{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14774,"databundleVersionId":875431,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import cohen_kappa_score\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport cv2\nfrom PIL import Image\nimport timm\nfrom multiprocessing import Pool\nfrom tqdm import tqdm\nfrom torchvision import transforms\nfrom torch.optim.lr_scheduler import LambdaLR","metadata":{"execution":{"iopub.status.busy":"2024-06-23T12:43:17.415679Z","iopub.execute_input":"2024-06-23T12:43:17.415992Z","iopub.status.idle":"2024-06-23T12:43:25.436424Z","shell.execute_reply.started":"2024-06-23T12:43:17.415967Z","shell.execute_reply":"2024-06-23T12:43:25.435376Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"## for removing file if required\nimport shutil\n\ntry:\n    shutil.rmtree(\"/kaggle/working/train\")\n    \n    model_file_to_delete =\"/kaggle/working/models\"\n\n    if os.path.isfile(model_file_to_delete):\n        os.remove(model_file_to_delete)\n    \nexcept:\n    print(\"No such directories\")","metadata":{"execution":{"iopub.status.busy":"2024-06-23T12:43:25.438248Z","iopub.execute_input":"2024-06-23T12:43:25.438538Z","iopub.status.idle":"2024-06-23T12:43:25.444284Z","shell.execute_reply.started":"2024-06-23T12:43:25.438513Z","shell.execute_reply":"2024-06-23T12:43:25.443387Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"No such directories\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Loading Data","metadata":{}},{"cell_type":"code","source":"def load_data(data_dir):\n    train_csv = os.path.join(data_dir, 'train.csv')\n    test_csv = os.path.join(data_dir, 'test.csv')\n    \n    train = pd.read_csv(train_csv)\n    test = pd.read_csv(test_csv)\n    \n    train_dir = os.path.join(data_dir, 'train_images/')\n    test_dir = os.path.join(data_dir, 'test_images/')\n    \n    train['file_path'] = train['id_code'].map(lambda x: os.path.join(train_dir, '{}.png'.format(x)))\n    test['file_path'] = test['id_code'].map(lambda x: os.path.join(test_dir, '{}.png'.format(x)))\n    \n    train['file_name'] = train[\"id_code\"] + \".png\"\n    test['file_name'] = test[\"id_code\"] + \".png\"\n    \n    train['diagnosis'] = train['diagnosis'].astype(str)\n    \n    return train, test","metadata":{"execution":{"iopub.status.busy":"2024-06-23T12:43:25.445570Z","iopub.execute_input":"2024-06-23T12:43:25.445883Z","iopub.status.idle":"2024-06-23T12:43:25.455650Z","shell.execute_reply.started":"2024-06-23T12:43:25.445860Z","shell.execute_reply":"2024-06-23T12:43:25.454781Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data_dir = '/kaggle/input/aptos2019-blindness-detection/'\ntrain_df, test_df = load_data(data_dir)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T12:43:25.457606Z","iopub.execute_input":"2024-06-23T12:43:25.457973Z","iopub.status.idle":"2024-06-23T12:43:25.505055Z","shell.execute_reply.started":"2024-06-23T12:43:25.457950Z","shell.execute_reply":"2024-06-23T12:43:25.504264Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"print(train_df.shape)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-23T12:43:25.506107Z","iopub.execute_input":"2024-06-23T12:43:25.506398Z","iopub.status.idle":"2024-06-23T12:43:25.521313Z","shell.execute_reply.started":"2024-06-23T12:43:25.506373Z","shell.execute_reply":"2024-06-23T12:43:25.520473Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"(3662, 4)\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"        id_code diagnosis                                          file_path  \\\n0  000c1434d8d7         2  /kaggle/input/aptos2019-blindness-detection/tr...   \n1  001639a390f0         4  /kaggle/input/aptos2019-blindness-detection/tr...   \n2  0024cdab0c1e         1  /kaggle/input/aptos2019-blindness-detection/tr...   \n3  002c21358ce6         0  /kaggle/input/aptos2019-blindness-detection/tr...   \n4  005b95c28852         0  /kaggle/input/aptos2019-blindness-detection/tr...   \n\n          file_name  \n0  000c1434d8d7.png  \n1  001639a390f0.png  \n2  0024cdab0c1e.png  \n3  002c21358ce6.png  \n4  005b95c28852.png  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id_code</th>\n      <th>diagnosis</th>\n      <th>file_path</th>\n      <th>file_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000c1434d8d7</td>\n      <td>2</td>\n      <td>/kaggle/input/aptos2019-blindness-detection/tr...</td>\n      <td>000c1434d8d7.png</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>001639a390f0</td>\n      <td>4</td>\n      <td>/kaggle/input/aptos2019-blindness-detection/tr...</td>\n      <td>001639a390f0.png</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0024cdab0c1e</td>\n      <td>1</td>\n      <td>/kaggle/input/aptos2019-blindness-detection/tr...</td>\n      <td>0024cdab0c1e.png</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>002c21358ce6</td>\n      <td>0</td>\n      <td>/kaggle/input/aptos2019-blindness-detection/tr...</td>\n      <td>002c21358ce6.png</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>005b95c28852</td>\n      <td>0</td>\n      <td>/kaggle/input/aptos2019-blindness-detection/tr...</td>\n      <td>005b95c28852.png</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(test_df.shape)\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-23T12:43:25.522329Z","iopub.execute_input":"2024-06-23T12:43:25.522578Z","iopub.status.idle":"2024-06-23T12:43:25.532929Z","shell.execute_reply.started":"2024-06-23T12:43:25.522556Z","shell.execute_reply":"2024-06-23T12:43:25.532043Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"(1928, 3)\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"        id_code                                          file_path  \\\n0  0005cfc8afb6  /kaggle/input/aptos2019-blindness-detection/te...   \n1  003f0afdcd15  /kaggle/input/aptos2019-blindness-detection/te...   \n2  006efc72b638  /kaggle/input/aptos2019-blindness-detection/te...   \n3  00836aaacf06  /kaggle/input/aptos2019-blindness-detection/te...   \n4  009245722fa4  /kaggle/input/aptos2019-blindness-detection/te...   \n\n          file_name  \n0  0005cfc8afb6.png  \n1  003f0afdcd15.png  \n2  006efc72b638.png  \n3  00836aaacf06.png  \n4  009245722fa4.png  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id_code</th>\n      <th>file_path</th>\n      <th>file_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0005cfc8afb6</td>\n      <td>/kaggle/input/aptos2019-blindness-detection/te...</td>\n      <td>0005cfc8afb6.png</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>003f0afdcd15</td>\n      <td>/kaggle/input/aptos2019-blindness-detection/te...</td>\n      <td>003f0afdcd15.png</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>006efc72b638</td>\n      <td>/kaggle/input/aptos2019-blindness-detection/te...</td>\n      <td>006efc72b638.png</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00836aaacf06</td>\n      <td>/kaggle/input/aptos2019-blindness-detection/te...</td>\n      <td>00836aaacf06.png</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>009245722fa4</td>\n      <td>/kaggle/input/aptos2019-blindness-detection/te...</td>\n      <td>009245722fa4.png</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Pre-Processing","metadata":{}},{"cell_type":"code","source":"def crop_img(img, percentage):\n    \n    img_arr = np.array(img)\n    img_gray = cv2.cvtColor(img_arr, cv2.COLOR_BGR2GRAY)\n    \n    threshold = img_gray > 0.1 * np.mean(img_gray[img_gray != 0])\n    row_sums = np.sum(threshold, axis=1)\n    col_sums = np.sum(threshold, axis=0)\n    \n    rows = np.where(row_sums > img_arr.shape[1] * percentage)[0]\n    cols = np.where(col_sums > img_arr.shape[0] * percentage)[0]\n    \n    min_row, min_col = np.min(rows), np.min(cols)\n    max_row, max_col = np.max(rows), np.max(cols)\n    \n    crop_img = img_arr[min_row : max_row + 1, min_col : max_col + 1]\n    \n    return Image.fromarray(crop_img)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T12:43:25.534050Z","iopub.execute_input":"2024-06-23T12:43:25.534302Z","iopub.status.idle":"2024-06-23T12:43:25.541895Z","shell.execute_reply.started":"2024-06-23T12:43:25.534281Z","shell.execute_reply":"2024-06-23T12:43:25.541092Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def resize_maintain_aspect(img, desired_size):\n    old_width, old_height = img.size\n    aspect_ratio = old_width / old_height\n\n    if aspect_ratio > 1:\n        new_width = desired_size\n        new_height = int(desired_size / aspect_ratio)\n    else:\n        new_height = desired_size\n        new_width = int(desired_size * aspect_ratio)\n\n    resized_img = img.resize((new_width, new_height), Image.ANTIALIAS)\n    \n    padded_image = Image.new(\"RGB\", (desired_size, desired_size))\n    x_offset = (desired_size - new_width) // 2\n    y_offset = (desired_size - new_height) // 2\n    padded_image.paste(resized_img, (x_offset, y_offset))\n    \n    return padded_image","metadata":{"execution":{"iopub.status.busy":"2024-06-23T12:43:25.543036Z","iopub.execute_input":"2024-06-23T12:43:25.543582Z","iopub.status.idle":"2024-06-23T12:43:25.552705Z","shell.execute_reply.started":"2024-06-23T12:43:25.543558Z","shell.execute_reply":"2024-06-23T12:43:25.551849Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def save_single(args):\n    image_path, output_path_folder, percentage, output_size = args\n    image = Image.open(image_path)\n    \n    # Display the image\n    #plt.imshow(image)\n    #plt.title('Original Image')\n    #plt.show()\n    \n    croped_img = crop_img(image,percentage)\n    image_resized = resize_maintain_aspect(croped_img, desired_size=output_size[0])\n    \n    #print(output_path_folder)\n    #print(image_path)\n    output_image_path = os.path.basename(image_path)\n    # Save the resized image\n    output_file_path = os.path.join(output_path_folder, output_image_path)\n    #print(output_file_path)\n    image_resized.save(output_file_path)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T12:43:25.553914Z","iopub.execute_input":"2024-06-23T12:43:25.554247Z","iopub.status.idle":"2024-06-23T12:43:25.561149Z","shell.execute_reply.started":"2024-06-23T12:43:25.554217Z","shell.execute_reply":"2024-06-23T12:43:25.560327Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def fast_image_resize(df, output_path_folder, percentage, output_size=None):\n    \"\"\"Uses multiprocessing to make it fast\"\"\"\n    if not output_size:\n        warnings.warn(\"Need to specify output_size! For example: output_size=100\")\n        return\n\n    if not os.path.exists(output_path_folder):\n        os.makedirs(output_path_folder)\n        \n    jobs = []\n    for df_item in range(len(df)):\n        image_path = df.file_path.iloc[df_item]\n        #print(image_path)\n        job = (image_path, output_path_folder, percentage, output_size)\n        jobs.append(job)\n    \n    \"\"\"\n    results = []\n    for job in tqdm(jobs, total=len(jobs)):\n        result = save_single(job)\n        results.append(result)\n    \"\"\"\n    with Pool() as p:\n        list(tqdm(p.imap_unordered(save_single, jobs), total=len(jobs)))","metadata":{"execution":{"iopub.status.busy":"2024-06-23T12:43:25.564603Z","iopub.execute_input":"2024-06-23T12:43:25.564880Z","iopub.status.idle":"2024-06-23T12:43:25.572513Z","shell.execute_reply.started":"2024-06-23T12:43:25.564858Z","shell.execute_reply":"2024-06-23T12:43:25.571653Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"percentage = 0.01\nfast_image_resize(train_df, \"/kaggle/working/train/images_resized_150/\",percentage, output_size=(100, 100))","metadata":{"execution":{"iopub.status.busy":"2024-06-23T12:43:25.573549Z","iopub.execute_input":"2024-06-23T12:43:25.573833Z","iopub.status.idle":"2024-06-23T12:48:55.254657Z","shell.execute_reply.started":"2024-06-23T12:43:25.573804Z","shell.execute_reply":"2024-06-23T12:48:55.253518Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"  0%|          | 0/3662 [00:00<?, ?it/s]/tmp/ipykernel_34/3153832717.py:12: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n  resized_img = img.resize((new_width, new_height), Image.ANTIALIAS)\n  0%|          | 1/3662 [00:00<13:33,  4.50it/s]/tmp/ipykernel_34/3153832717.py:12: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n  resized_img = img.resize((new_width, new_height), Image.ANTIALIAS)\n  0%|          | 2/3662 [00:00<13:56,  4.38it/s]/tmp/ipykernel_34/3153832717.py:12: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n  resized_img = img.resize((new_width, new_height), Image.ANTIALIAS)\n/tmp/ipykernel_34/3153832717.py:12: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n  resized_img = img.resize((new_width, new_height), Image.ANTIALIAS)\n100%|██████████| 3662/3662 [05:29<00:00, 11.11it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Model Implementation - Ensambling \n#### EfficientNet b0, b1, b2, b3","metadata":{}},{"cell_type":"code","source":"class BlindnessDataset(Dataset):\n    def __init__(self, csv_file, root_dir, transform=None, augmentations=None, max_count=None, test=False):\n        self.annotations = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n        #self.augmentations = augmentations\n        self.max_count = max_count\n        self.test = test\n        \n        if not test:\n            self.class_counts = self.annotations['diagnosis'].value_counts().sort_index()\n        else:\n            self.class_counts = None\n        \n        if max_count:\n            self.oversample(max_count)\n    \n    def oversample(self, max_count): ## Over sampling classes to balance\n        samples = []\n        for diagnosis in self.class_counts.index:\n            class_samples = self.annotations[self.annotations['diagnosis'] == diagnosis]\n            oversampled_class = class_samples.sample(max_count, replace=True)\n            samples.append(oversampled_class)\n        self.annotations = pd.concat(samples).reset_index(drop=True)\n    \n    def __len__(self):\n        return len(self.annotations)\n    \n    def __getitem__(self, idx):\n        img_name = os.path.join(self.root_dir, self.annotations.iloc[idx, 0] + '.png')\n        image = Image.open(img_name).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        if self.test:\n            return image\n                \n        label = int(self.annotations.iloc[idx, 1])\n        return image, label","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:06:23.877834Z","iopub.execute_input":"2024-06-23T13:06:23.878190Z","iopub.status.idle":"2024-06-23T13:06:23.890459Z","shell.execute_reply.started":"2024-06-23T13:06:23.878165Z","shell.execute_reply":"2024-06-23T13:06:23.889503Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"def select_model(model_name):\n    if model_name == 'efficientnet_b0':\n        return timm.create_model('efficientnet_b0', pretrained=True, num_classes=5)\n    elif model_name == 'efficientnet_b1':\n        return timm.create_model('efficientnet_b1', pretrained=True, num_classes=5)\n    elif model_name == 'efficientnet_b2':\n        return timm.create_model('efficientnet_b2', pretrained=True, num_classes=5)\n    elif model_name == 'efficientnet_b3':\n        return timm.create_model('efficientnet_b3', pretrained=True, num_classes=5)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:06:24.430326Z","iopub.execute_input":"2024-06-23T13:06:24.431019Z","iopub.status.idle":"2024-06-23T13:06:24.437090Z","shell.execute_reply.started":"2024-06-23T13:06:24.430981Z","shell.execute_reply":"2024-06-23T13:06:24.435979Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"def lr_schedule(epoch):\n    if epoch < 10:\n        return 5e-4\n    elif epoch < 16:\n        return 1e-4\n    elif epoch < 22:\n        return 1e-5\n    else:\n        return 1e-3\n        ","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:06:24.866542Z","iopub.execute_input":"2024-06-23T13:06:24.867002Z","iopub.status.idle":"2024-06-23T13:06:24.872681Z","shell.execute_reply.started":"2024-06-23T13:06:24.866970Z","shell.execute_reply":"2024-06-23T13:06:24.871514Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"def train_model(model_name,train_loader,valid_loader):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = select_model(model_name).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters())\n    \n    def lambda_epoch(epoch):\n        return lr_schedule(epoch)\n    \n    def quadratic_weighted_kappa(y_true, y_pred):\n        return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n    \n    scheduler = LambdaLR(optimizer,lr_lambda = lambda_epoch)\n    best_kappa = 0.0\n    \n    for epoch in range(25):\n        model.train()\n        for images, labels in train_loader:\n            if isinstance(images, Image.Image):  # Check if the image is a PIL image\n                transform = transforms.ToTensor()\n                images = transform(images)\n            else:\n                images = images\n                \n            images, labels = images.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n        scheduler.step()\n        \n        model.eval()\n        val_loss = 0\n        val_preds = []\n        val_labels_epoch = []\n        with torch.no_grad():\n            for images, labels in valid_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs,labels)\n                val_loss += loss.item()\n                preds = torch.argmax(outputs, dim=1)\n                val_labels_epoch.extend(labels.cpu().numpy())\n                val_preds.extend(preds.cpu().numpy())\n        \n        #val_predictions.append(val_preds)\n        #val_labels.append(val_labels_epoch)\n        \n        kappa = quadratic_weighted_kappa(val_labels_epoch, val_preds)\n        print(f\"Epoch {epoch+1}/{25}, Validation Loss: {val_loss / len(valid_loader)}\")\n        print(f\"Validation QWK: {kappa}\")\n        \n        if kappa > best_kappa:\n            best_kappa = kappa\n            best_model = model.state_dict()\n            \n    model.load_state_dict(best_model)\n    \n    return model\n\n    ","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:06:25.267285Z","iopub.execute_input":"2024-06-23T13:06:25.267986Z","iopub.status.idle":"2024-06-23T13:06:25.280707Z","shell.execute_reply.started":"2024-06-23T13:06:25.267953Z","shell.execute_reply":"2024-06-23T13:06:25.279741Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"csv_file = '/kaggle/input/aptos2019-blindness-detection/train.csv'\npros_img_dir = '/kaggle/working/train/images_resized_150'\ntest_csv_file = '/kaggle/input/aptos2019-blindness-detection/test.csv'\ntest_root_dir = '/kaggle/input/aptos2019-blindness-detection/test_images'\nval_csv_file = '/kaggle/working/val.csv'\nval_root_dir = '/kaggle/working/val_images/'\n\nos.makedirs(val_root_dir, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:06:25.631076Z","iopub.execute_input":"2024-06-23T13:06:25.631773Z","iopub.status.idle":"2024-06-23T13:06:25.636529Z","shell.execute_reply.started":"2024-06-23T13:06:25.631737Z","shell.execute_reply":"2024-06-23T13:06:25.635522Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([ \n        transforms.Resize((224, 224)), transforms.ToTensor() \n    ])","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:06:26.352670Z","iopub.execute_input":"2024-06-23T13:06:26.353356Z","iopub.status.idle":"2024-06-23T13:06:26.357838Z","shell.execute_reply.started":"2024-06-23T13:06:26.353322Z","shell.execute_reply":"2024-06-23T13:06:26.356893Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"## Loading the dataset for determination of the class counts\ntemp_dataset = BlindnessDataset(csv_file, pros_img_dir, transform=transform)\nclass_counts = temp_dataset.class_counts\nmax_count = class_counts.max()","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:06:26.873616Z","iopub.execute_input":"2024-06-23T13:06:26.874001Z","iopub.status.idle":"2024-06-23T13:06:26.884472Z","shell.execute_reply.started":"2024-06-23T13:06:26.873970Z","shell.execute_reply":"2024-06-23T13:06:26.883575Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"dataset = BlindnessDataset(csv_file, pros_img_dir, transform=transform, max_count=max_count)\ntrain_indices, val_indices = train_test_split(list(range(len(dataset))), test_size=0.2, stratify=dataset.annotations.iloc[:, 1])\ntrain_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\nval_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\ntrain_loader = DataLoader(dataset, batch_size=32, sampler=train_sampler)\nval_loader = DataLoader(dataset, batch_size=32, sampler=val_sampler)\n        ","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:06:27.647031Z","iopub.execute_input":"2024-06-23T13:06:27.647387Z","iopub.status.idle":"2024-06-23T13:06:27.668967Z","shell.execute_reply.started":"2024-06-23T13:06:27.647357Z","shell.execute_reply":"2024-06-23T13:06:27.668246Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"val_annotations = dataset.annotations.iloc[val_indices]\nval_annotations.to_csv(val_csv_file, index=False)\n\nfor idx in val_indices:\n    img_name = dataset.annotations.iloc[idx, 0] + '.png'\n    src_path = os.path.join(pros_img_dir, img_name)\n    dst_path = os.path.join(val_root_dir, img_name)\n    shutil.copy2(src_path, dst_path)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:06:28.435004Z","iopub.execute_input":"2024-06-23T13:06:28.435642Z","iopub.status.idle":"2024-06-23T13:06:28.767130Z","shell.execute_reply.started":"2024-06-23T13:06:28.435614Z","shell.execute_reply":"2024-06-23T13:06:28.766247Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"def cross_validate_and_ensemble(train_loader, val_loader): \n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    \n    models = {'efficientnet_b0': [], 'efficientnet_b1': [], \n              'efficientnet_b2': [], 'efficientnet_b3': []}\n        \n    for model_type in models.keys(): \n        model = train_model(model_type, train_loader, val_loader) \n        models[model_type].append(model) \n    return models","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:06:29.343555Z","iopub.execute_input":"2024-06-23T13:06:29.344218Z","iopub.status.idle":"2024-06-23T13:06:29.351066Z","shell.execute_reply.started":"2024-06-23T13:06:29.344184Z","shell.execute_reply":"2024-06-23T13:06:29.350073Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"def get_predicted_labels(predictions):\n    predicted_labels = np.argmax(predictions, axis=1)\n    return predicted_labels","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:06:30.109102Z","iopub.execute_input":"2024-06-23T13:06:30.109471Z","iopub.status.idle":"2024-06-23T13:06:30.114047Z","shell.execute_reply.started":"2024-06-23T13:06:30.109440Z","shell.execute_reply":"2024-06-23T13:06:30.113092Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"def load_ground_truth_labels(test_csv_file):\n    test_df = pd.read_csv(test_csv_file)\n    if test_df.shape[1] > 1:\n        ground_truth_labels = test_df.iloc[:, 1].values\n    else:\n        ground_truth_labels = None\n    return ground_truth_labels","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:06:30.568919Z","iopub.execute_input":"2024-06-23T13:06:30.569286Z","iopub.status.idle":"2024-06-23T13:06:30.574890Z","shell.execute_reply.started":"2024-06-23T13:06:30.569257Z","shell.execute_reply":"2024-06-23T13:06:30.573784Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"def quadratic_weighted_kappa(y_true, y_pred):\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:06:31.023011Z","iopub.execute_input":"2024-06-23T13:06:31.023373Z","iopub.status.idle":"2024-06-23T13:06:31.028007Z","shell.execute_reply.started":"2024-06-23T13:06:31.023344Z","shell.execute_reply":"2024-06-23T13:06:31.027055Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"def predict_ensemble(models, test_csv_file,test_root_dir): \n    transform = transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor() ]) \n    test_dataset = BlindnessDataset(test_csv_file, test_root_dir, transform=transform, test=True)\n    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n    predictions = [] \n    for model_type, model_list in models.items(): \n        for model in model_list: \n            model.eval() \n            model = model.to(device) \n            model_preds = [] \n            with torch.no_grad(): \n                for images in test_loader: \n                    images = images.to(device) \n                    outputs = model(images) \n                    preds = torch.softmax(outputs, dim=1) \n                    model_preds.append(preds.cpu().numpy()) \n            model_preds = np.concatenate(model_preds, axis=0)\n            print(f\"{model_type} model predictions shape: {model_preds.shape}\")\n            predictions.append(model_preds)\n    \n    print(f\"Number of model predictions: {len(predictions)}\")\n    for i, pred in enumerate(predictions):\n        print(f\"Shape of predictions from model {i+1}: {pred.shape}\")\n    \n    final_predictions = np.mean(predictions, axis=0)\n    print(f\"Final averaged predictions shape: {final_predictions.shape}\")\n    \n    predicted_labels = get_predicted_labels(final_predictions)\n    \n    ground_truth_labels = load_ground_truth_labels(test_csv_file)\n    \n    if ground_truth_labels is not None:\n        final_kappa = quadratic_weighted_kappa(ground_truth_labels, predicted_labels)\n        print(\"Final Kappa:\", final_kappa)\n    else:\n        final_kappa = None\n        print(\"Ground truth labels are not available in the test CSV file.\")\n    \n    return final_predictions, final_kappa","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:06:31.425687Z","iopub.execute_input":"2024-06-23T13:06:31.426070Z","iopub.status.idle":"2024-06-23T13:06:31.436846Z","shell.execute_reply.started":"2024-06-23T13:06:31.426039Z","shell.execute_reply":"2024-06-23T13:06:31.435918Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"models = cross_validate_and_ensemble(train_loader, val_loader)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:06:32.128590Z","iopub.execute_input":"2024-06-23T13:06:32.128957Z","iopub.status.idle":"2024-06-23T14:18:02.860702Z","shell.execute_reply.started":"2024-06-23T13:06:32.128928Z","shell.execute_reply":"2024-06-23T14:18:02.859780Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"Epoch 1/25, Validation Loss: 3.425243536631266\nValidation QWK: 0.10372731731240803\nEpoch 2/25, Validation Loss: 3.02617857749002\nValidation QWK: 0.20901126408010018\nEpoch 3/25, Validation Loss: 2.773119598104243\nValidation QWK: 0.2711651824909068\nEpoch 4/25, Validation Loss: 2.5814951867388007\nValidation QWK: 0.2971180821219077\nEpoch 5/25, Validation Loss: 2.4160844401309363\nValidation QWK: 0.3331246086412023\nEpoch 6/25, Validation Loss: 2.2259604345288193\nValidation QWK: 0.3806500761808025\nEpoch 7/25, Validation Loss: 2.1242343752007735\nValidation QWK: 0.41696383087111566\nEpoch 8/25, Validation Loss: 1.905723854115135\nValidation QWK: 0.4754076973255056\nEpoch 9/25, Validation Loss: 1.9038859647617006\nValidation QWK: 0.4579762057055137\nEpoch 10/25, Validation Loss: 1.7319067444717675\nValidation QWK: 0.5045472518782128\nEpoch 11/25, Validation Loss: 1.696681088522861\nValidation QWK: 0.5034366489430684\nEpoch 12/25, Validation Loss: 1.6847244375630428\nValidation QWK: 0.5126093669583118\nEpoch 13/25, Validation Loss: 1.6976171501895838\nValidation QWK: 0.5289902280130294\nEpoch 14/25, Validation Loss: 1.6643072230774059\nValidation QWK: 0.5319802376819335\nEpoch 15/25, Validation Loss: 1.6680244740686918\nValidation QWK: 0.5388139808875507\nEpoch 16/25, Validation Loss: 1.6325943815080743\nValidation QWK: 0.5339413680781759\nEpoch 17/25, Validation Loss: 1.6432745038417347\nValidation QWK: 0.5150519258577626\nEpoch 18/25, Validation Loss: 1.6357425001629613\nValidation QWK: 0.5399663168804248\nEpoch 19/25, Validation Loss: 1.6399144425726773\nValidation QWK: 0.5372151231721776\nEpoch 20/25, Validation Loss: 1.6076402047224212\nValidation QWK: 0.5238402894430805\nEpoch 21/25, Validation Loss: 1.6503051749446935\nValidation QWK: 0.5219179843167503\nEpoch 22/25, Validation Loss: 1.668105909698888\nValidation QWK: 0.5290488431876607\nEpoch 23/25, Validation Loss: 1.453132358559391\nValidation QWK: 0.5761780104712042\nEpoch 24/25, Validation Loss: 1.3058579731405826\nValidation QWK: 0.6479905746825501\nEpoch 25/25, Validation Loss: 1.184922274790312\nValidation QWK: 0.6669402270551223\nEpoch 1/25, Validation Loss: 4.208462229946203\nValidation QWK: 0.04987834549878345\nEpoch 2/25, Validation Loss: 3.766400044424492\nValidation QWK: 0.1437444543034605\nEpoch 3/25, Validation Loss: 3.510807882275498\nValidation QWK: 0.18697295725501595\nEpoch 4/25, Validation Loss: 3.2212669933051394\nValidation QWK: 0.22238350994338796\nEpoch 5/25, Validation Loss: 3.028877712132638\nValidation QWK: 0.30793559516963775\nEpoch 6/25, Validation Loss: 2.782652219136556\nValidation QWK: 0.32720428550745617\nEpoch 7/25, Validation Loss: 2.6140153909984387\nValidation QWK: 0.37011294526498695\nEpoch 8/25, Validation Loss: 2.41776120244411\nValidation QWK: 0.418941504178273\nEpoch 9/25, Validation Loss: 2.338625387141579\nValidation QWK: 0.43588252398953187\nEpoch 10/25, Validation Loss: 2.185932077859577\nValidation QWK: 0.4717218265605362\nEpoch 11/25, Validation Loss: 2.1498511151263586\nValidation QWK: 0.47662356117663773\nEpoch 12/25, Validation Loss: 2.1211202772040116\nValidation QWK: 0.5007181844297616\nEpoch 13/25, Validation Loss: 2.0959307520013106\nValidation QWK: 0.49850873455475075\nEpoch 14/25, Validation Loss: 2.1101380858504983\nValidation QWK: 0.4818528456432707\nEpoch 15/25, Validation Loss: 2.04485548483698\nValidation QWK: 0.5053852526926264\nEpoch 16/25, Validation Loss: 2.021609118110255\nValidation QWK: 0.5144596651445967\nEpoch 17/25, Validation Loss: 2.0182673052737585\nValidation QWK: 0.5165207568483479\nEpoch 18/25, Validation Loss: 2.050302534772639\nValidation QWK: 0.5104745617785378\nEpoch 19/25, Validation Loss: 2.021199245201914\nValidation QWK: 0.5174904404475287\nEpoch 20/25, Validation Loss: 2.0114438889319435\nValidation QWK: 0.5176935006344283\nEpoch 21/25, Validation Loss: 2.032172263714305\nValidation QWK: 0.5183951131472997\nEpoch 22/25, Validation Loss: 2.0062505048617982\nValidation QWK: 0.5208333333333333\nEpoch 23/25, Validation Loss: 1.7866127825619882\nValidation QWK: 0.5660483417650366\nEpoch 24/25, Validation Loss: 1.6096067815496211\nValidation QWK: 0.6096331559787175\nEpoch 2/25, Validation Loss: 2.808883179698074\nValidation QWK: 0.2548824892419729\nEpoch 3/25, Validation Loss: 2.4197911923391775\nValidation QWK: 0.3079629037416054\nEpoch 4/25, Validation Loss: 2.1708075770160606\nValidation QWK: 0.39688473520249223\nEpoch 5/25, Validation Loss: 1.9663031038485075\nValidation QWK: 0.4336460095340612\nEpoch 6/25, Validation Loss: 1.7360640534183436\nValidation QWK: 0.5155124246434348\nEpoch 7/25, Validation Loss: 1.6152258180735404\nValidation QWK: 0.5404011461318052\nEpoch 8/25, Validation Loss: 1.4793530765332674\nValidation QWK: 0.6048140043763677\nEpoch 9/25, Validation Loss: 1.367207201949337\nValidation QWK: 0.6437241182350422\nEpoch 10/25, Validation Loss: 1.2447966745025234\nValidation QWK: 0.6750035127160321\nEpoch 11/25, Validation Loss: 1.24090103726638\nValidation QWK: 0.6651641076207917\nEpoch 12/25, Validation Loss: 1.2397088586238392\nValidation QWK: 0.6578873634071168\nEpoch 13/25, Validation Loss: 1.2285472290557728\nValidation QWK: 0.6484069312465064\nEpoch 14/25, Validation Loss: 1.2006508404748482\nValidation QWK: 0.6834623504574244\nEpoch 15/25, Validation Loss: 1.1779306928316753\nValidation QWK: 0.6918395573997234\nEpoch 16/25, Validation Loss: 1.1327561694279051\nValidation QWK: 0.701295045045045\nEpoch 17/25, Validation Loss: 1.166985976068597\nValidation QWK: 0.7067584656456372\nEpoch 20/25, Validation Loss: 1.1478373439688432\nValidation QWK: 0.6926091825307951\nEpoch 21/25, Validation Loss: 1.1484803484197248\nValidation QWK: 0.7034098816979819\nEpoch 22/25, Validation Loss: 1.1439957514143826\nValidation QWK: 0.7024390243902439\nEpoch 23/25, Validation Loss: 0.9997762222039072\nValidation QWK: 0.7550416020307432\nEpoch 24/25, Validation Loss: 0.8858057762447157\nValidation QWK: 0.7921092564491654\nEpoch 25/25, Validation Loss: 0.7966059179682481\nValidation QWK: 0.814457166505725\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/49.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"514c8421626d43e5820852641b8ce092"}},"metadata":{}},{"name":"stdout","text":"Epoch 1/25, Validation Loss: 3.509630931051154\nValidation QWK: 0.0328232971372161\nEpoch 2/25, Validation Loss: 3.0375624393161975\nValidation QWK: 0.08704504963094939\nEpoch 3/25, Validation Loss: 2.7778269177988957\nValidation QWK: 0.12812248186946007\nEpoch 4/25, Validation Loss: 2.527450111874363\nValidation QWK: 0.24071635501711885\nEpoch 5/25, Validation Loss: 2.314150235109162\nValidation QWK: 0.2728838256722744\nEpoch 6/25, Validation Loss: 2.293821098511679\nValidation QWK: 0.3707986017746706\nEpoch 7/25, Validation Loss: 2.016940903245357\nValidation QWK: 0.41683313357304563\nEpoch 8/25, Validation Loss: 1.9282165054689373\nValidation QWK: 0.45103127987979785\nEpoch 9/25, Validation Loss: 1.785440420895292\nValidation QWK: 0.49138287605128916\nEpoch 10/25, Validation Loss: 1.7063665442299425\nValidation QWK: 0.5195847750865052\nEpoch 11/25, Validation Loss: 1.6952583319262455\nValidation QWK: 0.5369660460021906\nEpoch 12/25, Validation Loss: 1.655021259659215\nValidation QWK: 0.5358082836958027\nEpoch 13/25, Validation Loss: 1.6082921645097565\nValidation QWK: 0.5323183391003461\nEpoch 14/25, Validation Loss: 1.6166114692102398\nValidation QWK: 0.5425797503467407\nEpoch 15/25, Validation Loss: 1.6453691605936016\nValidation QWK: 0.5548030407740152\nEpoch 16/25, Validation Loss: 1.5348602052320515\nValidation QWK: 0.5645562620956593\nEpoch 17/25, Validation Loss: 1.5984295219705815\nValidation QWK: 0.5687022900763359\nEpoch 18/25, Validation Loss: 1.5636107534692998\nValidation QWK: 0.5709141274238227\nEpoch 19/25, Validation Loss: 1.6606594721476238\nValidation QWK: 0.5683106575963719\nEpoch 20/25, Validation Loss: 1.5488335584339343\nValidation QWK: 0.5596059113300493\nEpoch 21/25, Validation Loss: 1.616318210175163\nValidation QWK: 0.5595271601463552\nEpoch 22/25, Validation Loss: 1.5804050449739422\nValidation QWK: 0.5661948376353039\nEpoch 23/25, Validation Loss: 1.4428480612604242\nValidation QWK: 0.6092195600392323\nEpoch 24/25, Validation Loss: 1.3883621567174007\nValidation QWK: 0.6652852603950822\nEpoch 25/25, Validation Loss: 1.2265437485878927\nValidation QWK: 0.6895784054542925\n","output_type":"stream"}]},{"cell_type":"code","source":"###DUE\nfinal_predictions_val, final_kappa_val = predict_ensemble(models, val_csv_file,val_root_dir)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T14:18:02.862750Z","iopub.execute_input":"2024-06-23T14:18:02.863056Z","iopub.status.idle":"2024-06-23T14:18:24.388150Z","shell.execute_reply.started":"2024-06-23T14:18:02.863030Z","shell.execute_reply":"2024-06-23T14:18:24.387071Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"efficientnet_b0 model predictions shape: (1805, 5)\nefficientnet_b1 model predictions shape: (1805, 5)\nefficientnet_b2 model predictions shape: (1805, 5)\nefficientnet_b3 model predictions shape: (1805, 5)\nNumber of model predictions: 4\nShape of predictions from model 1: (1805, 5)\nShape of predictions from model 2: (1805, 5)\nShape of predictions from model 3: (1805, 5)\nShape of predictions from model 4: (1805, 5)\nFinal averaged predictions shape: (1805, 5)\nFinal Kappa: 0.8510754897931223\n","output_type":"stream"}]},{"cell_type":"code","source":"final_predictions, final_kappa = predict_ensemble(models, test_csv_file,test_root_dir)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T12:48:55.811194Z","iopub.status.idle":"2024-06-23T12:48:55.811635Z","shell.execute_reply.started":"2024-06-23T12:48:55.811409Z","shell.execute_reply":"2024-06-23T12:48:55.811427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"thresholds = [0.5, 1.5, 2.5, 3.5]\nfinal_classes = np.digitize(predictions, bins=thresholds)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T12:48:55.813551Z","iopub.status.idle":"2024-06-23T12:48:55.813933Z","shell.execute_reply.started":"2024-06-23T12:48:55.813752Z","shell.execute_reply":"2024-06-23T12:48:55.813773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_predictions(final_classes)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T12:48:55.815572Z","iopub.status.idle":"2024-06-23T12:48:55.815944Z","shell.execute_reply.started":"2024-06-23T12:48:55.815770Z","shell.execute_reply":"2024-06-23T12:48:55.815785Z"},"trusted":true},"execution_count":null,"outputs":[]}]}